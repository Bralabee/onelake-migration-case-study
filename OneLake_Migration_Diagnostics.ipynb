{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0ba297",
   "metadata": {},
   "source": [
    "# OneLake Migration Diagnostics & Analysis\n",
    "\n",
    "## üîç Purpose\n",
    "This notebook analyzes the OneLake migration issues and provides diagnostic tools to troubleshoot why the migration is failing despite directories being created successfully.\n",
    "\n",
    "## üéØ Key Objectives\n",
    "1. **Validate Directory Structure** - Confirm all directories were created correctly\n",
    "2. **Analyze Migration Logs** - Identify the root cause of 0% success rates\n",
    "3. **Test API Connectivity** - Verify authentication and API access paths\n",
    "4. **Performance Analysis** - Generate reports on migration progress and bottlenecks\n",
    "5. **Path Mapping Validation** - Ensure directory paths match between creation and migration scripts\n",
    "\n",
    "## üìä Current Status\n",
    "- **Directories Created**: ‚úÖ 84 directories successfully created via Fabric notebook\n",
    "- **Migration Status**: ‚ùå 0% success rate in Python migration script\n",
    "- **Issue**: Path mismatch or API access problem between directory creation and file upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a4fa4",
   "metadata": {},
   "source": [
    "## üìö Section 1: Import Required Libraries\n",
    "Import necessary libraries for analysis, visualization, and API testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef59769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging for better output\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Using pandas version: {pd.__version__}\")\n",
    "print(f\"üìà Using matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"üé® Using seaborn version: {sns.__version__}\")\n",
    "print(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08dcda0",
   "metadata": {},
   "source": [
    "## üìÅ Section 2: Load Migration Configuration and Environment\n",
    "Load the onelake_directories.json file and environment variables to understand the migration setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load migration configuration and environment\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_environment():\n",
    "    \"\"\"Load environment variables with multi-path checking\"\"\"\n",
    "    env_paths = [\n",
    "        \"config/.env\",\n",
    "        \".env\",\n",
    "        \"../.env\",\n",
    "        \"../../config/.env\",\n",
    "        \"../config/.env\"\n",
    "    ]\n",
    "    \n",
    "    for env_path in env_paths:\n",
    "        if os.path.exists(env_path):\n",
    "            print(f\"üìÑ Loading environment from: {env_path}\")\n",
    "            with open(env_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if '=' in line and not line.strip().startswith('#'):\n",
    "                        key, value = line.strip().split('=', 1)\n",
    "                        os.environ[key] = value\n",
    "            return env_path\n",
    "    \n",
    "    print(\"‚ö†Ô∏è No .env file found in any expected location\")\n",
    "    return None\n",
    "\n",
    "# Load environment\n",
    "env_file_used = load_environment()\n",
    "\n",
    "# Load onelake_directories.json\n",
    "directories_file = \"onelake_directories.json\"\n",
    "if os.path.exists(directories_file):\n",
    "    with open(directories_file, 'r') as f:\n",
    "        onelake_dirs = json.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(onelake_dirs)} OneLake directories from {directories_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå {directories_file} not found\")\n",
    "    onelake_dirs = {}\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"\\nüîß Configuration Summary:\")\n",
    "print(f\"Environment file: {env_file_used}\")\n",
    "print(f\"Workspace ID: {os.environ.get('FABRIC_WORKSPACE_ID', 'Not set')}\")\n",
    "print(f\"Lakehouse ID: {os.environ.get('FABRIC_LAKEHOUSE_ID', 'Not set')}\")\n",
    "print(f\"OneLake directories loaded: {len(onelake_dirs)}\")\n",
    "\n",
    "# Show sample directory structure\n",
    "if onelake_dirs:\n",
    "    print(\"\\nüìÇ Sample directory paths:\")\n",
    "    for i, (key, path) in enumerate(list(onelake_dirs.items())[:5]):\n",
    "        print(f\"  {i+1}. {key}: {path}\")\n",
    "    if len(onelake_dirs) > 5:\n",
    "        print(f\"  ... and {len(onelake_dirs) - 5} more directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d4bec",
   "metadata": {},
   "source": [
    "## üîç Section 3: Analyze Directory Creation vs Migration Results\n",
    "Compare the successfully created directories with migration attempts to identify discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840bd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze migration logs and compare with created directories\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_migration_logs():\n",
    "    \"\"\"Analyze migration log files to understand failure patterns\"\"\"\n",
    "    log_files = glob.glob(\"*.log\") + glob.glob(\"logs/*.log\")\n",
    "    migration_logs = []\n",
    "    \n",
    "    print(\"üìã Analyzing migration logs...\")\n",
    "    for log_file in log_files:\n",
    "        try:\n",
    "            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                if 'migration' in content.lower() or 'upload' in content.lower():\n",
    "                    migration_logs.append({\n",
    "                        'file': log_file,\n",
    "                        'size': os.path.getsize(log_file),\n",
    "                        'modified': datetime.fromtimestamp(os.path.getmtime(log_file)),\n",
    "                        'content_preview': content[:500]\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {log_file}: {e}\")\n",
    "    \n",
    "    return migration_logs\n",
    "\n",
    "# Load and analyze migration progress file\n",
    "progress_file = \"migration_progress_optimized.json\"\n",
    "migration_progress = {}\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as f:\n",
    "        migration_progress = json.load(f)\n",
    "    print(f\"‚úÖ Loaded migration progress: {len(migration_progress)} entries\")\n",
    "else:\n",
    "    print(f\"‚ùå {progress_file} not found\")\n",
    "\n",
    "# Analyze migration logs\n",
    "logs = analyze_migration_logs()\n",
    "print(f\"üìã Found {len(logs)} migration-related log files\")\n",
    "\n",
    "# Compare directory creation vs migration attempts\n",
    "if onelake_dirs and migration_progress:\n",
    "    created_dirs = set(onelake_dirs.keys())\n",
    "    attempted_migrations = set(migration_progress.keys())\n",
    "    \n",
    "    print(f\"\\nüìä Directory Analysis:\")\n",
    "    print(f\"Directories created: {len(created_dirs)}\")\n",
    "    print(f\"Migration attempts: {len(attempted_migrations)}\")\n",
    "    print(f\"Overlap: {len(created_dirs.intersection(attempted_migrations))}\")\n",
    "    \n",
    "    # Check for successful migrations\n",
    "    successful_migrations = [k for k, v in migration_progress.items() if v.get('success', False)]\n",
    "    print(f\"Successful migrations: {len(successful_migrations)}\")\n",
    "    \n",
    "    if successful_migrations:\n",
    "        print(\"‚úÖ Sample successful migrations:\")\n",
    "        for success in successful_migrations[:3]:\n",
    "            print(f\"  - {success}\")\n",
    "    else:\n",
    "        print(\"‚ùå No successful migrations found\")\n",
    "        \n",
    "        # Analyze failure patterns\n",
    "        failure_reasons = {}\n",
    "        for k, v in migration_progress.items():\n",
    "            error = v.get('error', 'Unknown error')\n",
    "            failure_reasons[error] = failure_reasons.get(error, 0) + 1\n",
    "        \n",
    "        print(\"\\nüî¥ Failure patterns:\")\n",
    "        for reason, count in sorted(failure_reasons.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {count:3d}x: {reason}\")\n",
    "\n",
    "# Show recent log entries\n",
    "if logs:\n",
    "    latest_log = max(logs, key=lambda x: x['modified'])\n",
    "    print(f\"\\nüìÑ Latest migration log ({latest_log['file']}):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(latest_log['content_preview'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e46d3",
   "metadata": {},
   "source": [
    "## üîó Section 4: API Path and Authentication Analysis\n",
    "Compare the different API approaches used for directory creation vs file uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc51bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare API paths and authentication methods\n",
    "import requests\n",
    "\n",
    "def test_fabric_authentication():\n",
    "    \"\"\"Test different authentication approaches\"\"\"\n",
    "    workspace_id = os.environ.get('FABRIC_WORKSPACE_ID')\n",
    "    lakehouse_id = os.environ.get('FABRIC_LAKEHOUSE_ID')\n",
    "    access_token = os.environ.get('FABRIC_ACCESS_TOKEN')\n",
    "    \n",
    "    print(\"üîê Authentication Test:\")\n",
    "    print(f\"Workspace ID: {'‚úÖ' if workspace_id else '‚ùå'} {workspace_id}\")\n",
    "    print(f\"Lakehouse ID: {'‚úÖ' if lakehouse_id else '‚ùå'} {lakehouse_id}\")\n",
    "    print(f\"Access Token: {'‚úÖ' if access_token else '‚ùå'} {'Set' if access_token else 'Not set'}\")\n",
    "    \n",
    "    if not all([workspace_id, lakehouse_id, access_token]):\n",
    "        print(\"‚ùå Missing required authentication parameters\")\n",
    "        return False\n",
    "    \n",
    "    # Test API endpoints\n",
    "    test_results = {}\n",
    "    \n",
    "    # Test 1: Fabric API workspace access\n",
    "    try:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {access_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}\"\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        test_results['workspace_api'] = {\n",
    "            'status': response.status_code,\n",
    "            'success': response.status_code == 200,\n",
    "            'url': url\n",
    "        }\n",
    "        print(f\"Workspace API: {'‚úÖ' if response.status_code == 200 else '‚ùå'} {response.status_code}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_results['workspace_api'] = {'error': str(e), 'success': False}\n",
    "        print(f\"Workspace API: ‚ùå {e}\")\n",
    "    \n",
    "    # Test 2: OneLake API access\n",
    "    try:\n",
    "        onelake_url = f\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}.Lakehouse/Files\"\n",
    "        response = requests.get(onelake_url, headers=headers, timeout=30)\n",
    "        test_results['onelake_api'] = {\n",
    "            'status': response.status_code,\n",
    "            'success': response.status_code in [200, 404],  # 404 is expected for empty directory\n",
    "            'url': onelake_url\n",
    "        }\n",
    "        print(f\"OneLake API: {'‚úÖ' if response.status_code in [200, 404] else '‚ùå'} {response.status_code}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_results['onelake_api'] = {'error': str(e), 'success': False}\n",
    "        print(f\"OneLake API: ‚ùå {e}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def compare_api_paths():\n",
    "    \"\"\"Compare different API path formats\"\"\"\n",
    "    workspace_id = os.environ.get('FABRIC_WORKSPACE_ID')\n",
    "    lakehouse_id = os.environ.get('FABRIC_LAKEHOUSE_ID')\n",
    "    \n",
    "    if not workspace_id or not lakehouse_id:\n",
    "        print(\"‚ùå Missing workspace or lakehouse ID for path comparison\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîó API Path Comparison:\")\n",
    "    \n",
    "    # Path formats used in different contexts\n",
    "    paths = {\n",
    "        'Fabric Notebook (mssparkutils)': f\"abfss://COE_F_EUC_P2@onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}.Lakehouse/Files/\",\n",
    "        'OneLake REST API': f\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}.Lakehouse/Files/\",\n",
    "        'Fabric REST API': f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items/{lakehouse_id}/\",\n",
    "        'Azure Data Lake Gen2': f\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}.Lakehouse/Files/\"\n",
    "    }\n",
    "    \n",
    "    for method, path in paths.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  {path}\")\n",
    "    \n",
    "    # Show sample directory paths from onelake_directories.json\n",
    "    if onelake_dirs:\n",
    "        print(f\"\\nüìÇ Sample created directory paths:\")\n",
    "        sample_dirs = list(onelake_dirs.items())[:3]\n",
    "        for key, path in sample_dirs:\n",
    "            print(f\"  {key}:\")\n",
    "            print(f\"    Created: {path}\")\n",
    "            \n",
    "            # Show what the migration API would use\n",
    "            if workspace_id in path and lakehouse_id in path:\n",
    "                rest_path = path.replace(\"abfss://\", \"https://\").replace(\"@onelake.dfs.fabric.microsoft.com\", \".dfs.fabric.microsoft.com\")\n",
    "                print(f\"    REST API: {rest_path}\")\n",
    "\n",
    "# Run authentication tests\n",
    "auth_results = test_fabric_authentication()\n",
    "\n",
    "# Compare API paths\n",
    "compare_api_paths()\n",
    "\n",
    "# Summary of findings\n",
    "print(f\"\\nüìã API Analysis Summary:\")\n",
    "if auth_results:\n",
    "    working_apis = sum(1 for result in auth_results.values() if result.get('success', False))\n",
    "    print(f\"Working API endpoints: {working_apis}/{len(auth_results)}\")\n",
    "else:\n",
    "    print(\"‚ùå Authentication test failed - missing credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23dd39",
   "metadata": {},
   "source": [
    "## üß™ Section 5: Test File Upload to Identify Issues\n",
    "Perform a controlled test upload to identify the exact failure point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff88159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test file upload to identify exact failure points\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "def create_test_file():\n",
    "    \"\"\"Create a small test file for upload testing\"\"\"\n",
    "    test_content = f\"Test file created at {datetime.now()}\\nThis is a diagnostic test file.\"\n",
    "    \n",
    "    # Create temporary file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "        f.write(test_content)\n",
    "        return f.name, test_content\n",
    "\n",
    "def test_onelake_upload(test_file_path, test_content):\n",
    "    \"\"\"Test uploading a file to OneLake using REST API\"\"\"\n",
    "    workspace_id = os.environ.get('FABRIC_WORKSPACE_ID')\n",
    "    lakehouse_id = os.environ.get('FABRIC_LAKEHOUSE_ID')\n",
    "    access_token = os.environ.get('FABRIC_ACCESS_TOKEN')\n",
    "    \n",
    "    if not all([workspace_id, lakehouse_id, access_token]):\n",
    "        print(\"‚ùå Missing required credentials for upload test\")\n",
    "        return False\n",
    "    \n",
    "    # Choose a test directory that we know exists\n",
    "    if onelake_dirs:\n",
    "        test_dir_key = list(onelake_dirs.keys())[0]\n",
    "        test_dir_path = onelake_dirs[test_dir_key]\n",
    "        print(f\"üéØ Testing upload to: {test_dir_key}\")\n",
    "        print(f\"Directory path: {test_dir_path}\")\n",
    "    else:\n",
    "        print(\"‚ùå No OneLake directories available for testing\")\n",
    "        return False\n",
    "    \n",
    "    # Construct upload URL - try different approaches\n",
    "    test_results = {}\n",
    "    \n",
    "    # Approach 1: Direct OneLake API\n",
    "    try:\n",
    "        # Convert abfss path to https path\n",
    "        if test_dir_path.startswith(\"abfss://\"):\n",
    "            # Transform: abfss://COE_F_EUC_P2@onelake.dfs.fabric.microsoft.com/workspace/lakehouse.Lakehouse/Files/path\n",
    "            # To: https://onelake.dfs.fabric.microsoft.com/workspace/lakehouse.Lakehouse/Files/path\n",
    "            https_path = test_dir_path.replace(\"abfss://COE_F_EUC_P2@\", \"https://\")\n",
    "        else:\n",
    "            https_path = test_dir_path\n",
    "        \n",
    "        upload_url = f\"{https_path}/test_diagnostic_file.txt\"\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {access_token}',\n",
    "            'Content-Type': 'text/plain',\n",
    "            'x-ms-blob-type': 'BlockBlob'\n",
    "        }\n",
    "        \n",
    "        print(f\"üîÑ Testing upload to: {upload_url}\")\n",
    "        response = requests.put(upload_url, data=test_content, headers=headers, timeout=30)\n",
    "        \n",
    "        test_results['direct_upload'] = {\n",
    "            'url': upload_url,\n",
    "            'status': response.status_code,\n",
    "            'success': response.status_code in [200, 201],\n",
    "            'response': response.text[:200] if response.text else 'No response text'\n",
    "        }\n",
    "        \n",
    "        print(f\"Direct upload result: {'‚úÖ' if response.status_code in [200, 201] else '‚ùå'} {response.status_code}\")\n",
    "        if response.status_code not in [200, 201]:\n",
    "            print(f\"Error response: {response.text[:200]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        test_results['direct_upload'] = {'error': str(e), 'success': False}\n",
    "        print(f\"Direct upload error: ‚ùå {e}\")\n",
    "    \n",
    "    # Approach 2: Try Azure Data Lake Gen2 API format\n",
    "    try:\n",
    "        # Use Azure Data Lake Gen2 API pattern\n",
    "        gen2_url = f\"https://onelake.dfs.fabric.microsoft.com/{workspace_id}/{lakehouse_id}.Lakehouse/Files/{test_dir_key}/test_diagnostic_file.txt\"\n",
    "        \n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {access_token}',\n",
    "            'Content-Type': 'application/octet-stream'\n",
    "        }\n",
    "        \n",
    "        # First create the file\n",
    "        create_response = requests.put(gen2_url, headers=headers, timeout=30)\n",
    "        \n",
    "        if create_response.status_code in [200, 201]:\n",
    "            # Then append data\n",
    "            append_url = f\"{gen2_url}?action=append&position=0\"\n",
    "            append_response = requests.patch(append_url, data=test_content.encode(), headers=headers, timeout=30)\n",
    "            \n",
    "            if append_response.status_code in [200, 202]:\n",
    "                # Finally flush\n",
    "                flush_url = f\"{gen2_url}?action=flush&position={len(test_content.encode())}\"\n",
    "                flush_response = requests.patch(flush_url, headers=headers, timeout=30)\n",
    "                final_status = flush_response.status_code\n",
    "            else:\n",
    "                final_status = append_response.status_code\n",
    "        else:\n",
    "            final_status = create_response.status_code\n",
    "        \n",
    "        test_results['gen2_upload'] = {\n",
    "            'url': gen2_url,\n",
    "            'status': final_status,\n",
    "            'success': final_status in [200, 201],\n",
    "            'create_status': create_response.status_code,\n",
    "            'response': create_response.text[:200] if create_response.text else 'No response text'\n",
    "        }\n",
    "        \n",
    "        print(f\"Gen2 API upload result: {'‚úÖ' if final_status in [200, 201] else '‚ùå'} {final_status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_results['gen2_upload'] = {'error': str(e), 'success': False}\n",
    "        print(f\"Gen2 upload error: ‚ùå {e}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Create test file and run upload tests\n",
    "print(\"üß™ Creating test file for upload diagnosis...\")\n",
    "test_file_path, test_content = create_test_file()\n",
    "print(f\"‚úÖ Test file created: {test_file_path}\")\n",
    "print(f\"Content length: {len(test_content)} bytes\")\n",
    "\n",
    "# Run upload tests\n",
    "if auth_results and any(result.get('success', False) for result in auth_results.values()):\n",
    "    print(\"\\nüöÄ Running upload tests...\")\n",
    "    upload_results = test_onelake_upload(test_file_path, test_content)\n",
    "    \n",
    "    # Summarize upload test results\n",
    "    if upload_results:\n",
    "        print(f\"\\nüìä Upload Test Summary:\")\n",
    "        successful_methods = [method for method, result in upload_results.items() if result.get('success', False)]\n",
    "        print(f\"Successful upload methods: {len(successful_methods)}/{len(upload_results)}\")\n",
    "        \n",
    "        if successful_methods:\n",
    "            print(f\"‚úÖ Working methods: {', '.join(successful_methods)}\")\n",
    "        else:\n",
    "            print(\"‚ùå No upload methods worked\")\n",
    "            print(\"\\nüîç Failure details:\")\n",
    "            for method, result in upload_results.items():\n",
    "                if 'error' in result:\n",
    "                    print(f\"  {method}: {result['error']}\")\n",
    "                else:\n",
    "                    print(f\"  {method}: HTTP {result.get('status', 'Unknown')} - {result.get('response', 'No details')}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping upload tests - authentication failed\")\n",
    "\n",
    "# Clean up test file\n",
    "try:\n",
    "    os.unlink(test_file_path)\n",
    "    print(f\"\\nüßπ Cleaned up test file: {test_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not clean up test file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed97665",
   "metadata": {},
   "source": [
    "## üìù Section 6: Recommendations and Next Steps\n",
    "Based on the diagnostic analysis, provide actionable recommendations to fix the migration issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa11402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on diagnostic results\n",
    "def generate_recommendations():\n",
    "    \"\"\"Generate actionable recommendations based on the diagnostic analysis\"\"\"\n",
    "    \n",
    "    print(\"üéØ MIGRATION DIAGNOSTIC SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check what we discovered\n",
    "    has_directories = len(onelake_dirs) > 0\n",
    "    has_migration_progress = len(migration_progress) > 0\n",
    "    has_auth = auth_results and any(result.get('success', False) for result in auth_results.values())\n",
    "    \n",
    "    print(f\"‚úÖ OneLake directories created: {len(onelake_dirs)} directories\")\n",
    "    print(f\"{'‚úÖ' if has_migration_progress else '‚ùå'} Migration progress file: {'Found' if has_migration_progress else 'Not found'}\")\n",
    "    print(f\"{'‚úÖ' if has_auth else '‚ùå'} API authentication: {'Working' if has_auth else 'Failed'}\")\n",
    "    \n",
    "    # Analyze the gap\n",
    "    print(f\"\\nüîç ROOT CAUSE ANALYSIS:\")\n",
    "    \n",
    "    if has_directories and not has_migration_progress:\n",
    "        print(\"1. ‚ùå ISSUE: Migration progress file missing or empty\")\n",
    "        print(\"   ‚Üí The migration script may not be running or saving progress\")\n",
    "        print(\"   ‚Üí Check if the migration script is using the correct file paths\")\n",
    "    \n",
    "    if has_directories and has_migration_progress:\n",
    "        failed_migrations = [k for k, v in migration_progress.items() if not v.get('success', False)]\n",
    "        print(f\"2. ‚ùå ISSUE: {len(failed_migrations)} migrations failed despite directories existing\")\n",
    "        print(\"   ‚Üí API path mismatch between directory creation and file upload\")\n",
    "        print(\"   ‚Üí Directory creation uses mssparkutils (abfss://) but upload uses REST API (https://)\")\n",
    "    \n",
    "    if not has_auth:\n",
    "        print(\"3. ‚ùå ISSUE: API authentication failing\")\n",
    "        print(\"   ‚Üí Access token may be expired or invalid\")\n",
    "        print(\"   ‚Üí Workspace/Lakehouse IDs may be incorrect\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDED SOLUTIONS:\")\n",
    "    print(\"1. üîß PATH STANDARDIZATION:\")\n",
    "    print(\"   ‚Üí Modify migration script to use consistent API approach\")\n",
    "    print(\"   ‚Üí Convert abfss:// paths to https:// for REST API uploads\")\n",
    "    print(\"   ‚Üí Test path conversion: abfss://COE_F_EUC_P2@onelake.dfs.fabric.microsoft.com ‚Üí https://onelake.dfs.fabric.microsoft.com\")\n",
    "    \n",
    "    print(\"\\\\n2. üîê AUTHENTICATION REFRESH:\")\n",
    "    print(\"   ‚Üí Regenerate Fabric access token\")\n",
    "    print(\"   ‚Üí Verify workspace and lakehouse IDs are correct\")\n",
    "    print(\"   ‚Üí Test authentication with simple API call before bulk migration\")\n",
    "    \n",
    "    print(\"\\\\n3. üìÅ MIGRATION STRATEGY:\")\n",
    "    print(\"   ‚Üí Use Azure Data Lake Gen2 API for file uploads (create ‚Üí append ‚Üí flush)\")\n",
    "    print(\"   ‚Üí Implement proper error handling and retry logic\")\n",
    "    print(\"   ‚Üí Add progress tracking with detailed logging\")\n",
    "    \n",
    "    print(\"\\\\n4. üß™ TESTING APPROACH:\")\n",
    "    print(\"   ‚Üí Start with single file upload test\")\n",
    "    print(\"   ‚Üí Verify directory listing works before file upload\")\n",
    "    print(\"   ‚Üí Implement batch upload with progress monitoring\")\n",
    "    \n",
    "    # Generate specific code fixes\n",
    "    print(f\"\\\\nüõ†Ô∏è SPECIFIC CODE CHANGES NEEDED:\")\n",
    "    \n",
    "    if onelake_dirs:\n",
    "        sample_dir = list(onelake_dirs.items())[0]\n",
    "        abfss_path = sample_dir[1]\n",
    "        https_path = abfss_path.replace(\"abfss://COE_F_EUC_P2@\", \"https://\")\n",
    "        \n",
    "        print(\"\\\\nPath conversion example:\")\n",
    "        print(f\"  Original (mssparkutils): {abfss_path}\")\n",
    "        print(f\"  For REST API:           {https_path}\")\n",
    "        \n",
    "        print(\"\\\\nRecommended upload function:\")\n",
    "        print(\"\"\"\n",
    "def upload_to_onelake_fixed(file_path, destination_path):\n",
    "    # Convert abfss path to https\n",
    "    if destination_path.startswith(\"abfss://\"):\n",
    "        api_path = destination_path.replace(\"abfss://COE_F_EUC_P2@\", \"https://\")\n",
    "    else:\n",
    "        api_path = destination_path\n",
    "    \n",
    "    # Use Azure Data Lake Gen2 API pattern\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    \n",
    "    # Step 1: Create file\n",
    "    create_response = requests.put(api_path, headers=headers)\n",
    "    \n",
    "    # Step 2: Append data\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    append_url = f\"{api_path}?action=append&position=0\"\n",
    "    append_response = requests.patch(append_url, data=data, headers=headers)\n",
    "    \n",
    "    # Step 3: Flush\n",
    "    flush_url = f\"{api_path}?action=flush&position={len(data)}\"\n",
    "    flush_response = requests.patch(flush_url, headers=headers)\n",
    "    \n",
    "    return flush_response.status_code in [200, 201]\n",
    "        \"\"\")\n",
    "\n",
    "# Generate final recommendations\n",
    "print(\"üî¨ DIAGNOSTIC ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "generate_recommendations()\n",
    "\n",
    "print(f\"\\\\n‚è∞ Analysis completed at: {datetime.now()}\")\n",
    "print(\"\\\\nüìã NEXT STEPS:\")\n",
    "print(\"1. Run this diagnostic notebook to identify specific issues\")\n",
    "print(\"2. Implement the recommended path conversion fixes\")\n",
    "print(\"3. Test single file upload before bulk migration\")\n",
    "print(\"4. Monitor progress with detailed logging\")\n",
    "print(\"\\\\nüéØ Expected outcome: 100% migration success rate with proper path handling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aca_taskforce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
